{"cells":[{"cell_type":"markdown","source":["**Pregunta N°2 - Segundo Parcial - SIS420**\n","\n","**Introducción.**\n","\n","En este cuadernillo se realizará el aprendizaje por refuerzo, esto aplicado a un cuatro en raya.\n","\n","**Objetivo.**\n","\n","Generar las tablas respectivas para que el modelo aprenda y logre jugar al cuatro en raya.\n","\n"],"metadata":{"id":"oTy_J2D3n-G7"}},{"cell_type":"markdown","source":["# Creación de los elementos del Juego y del Agente"],"metadata":{"id":"MFodsfLsqSXD"}},{"cell_type":"markdown","source":["**Modelo del entorno:** El entorno es todo aquello con lo que el agente interactúa y sobre lo cual tiene poco o ningún control directo. En el código, la clase Board representa el entorno porque mantiene el estado del juego, que es externo a los agentes y dicta las reglas del juego. Los agentes no pueden cambiar las reglas, solo pueden actuar dentro de ellas. Por lo tanto, Board es el modelo del entorno ya que define cómo se representa el estado del juego (el tablero de 7x6), las acciones válidas (movimientos posibles) y las condiciones de terminación del juego (victoria, derrota o empate).\n","\n","**Agente:** Un agente es una entidad que toma decisiones y realiza acciones dentro del entorno para alcanzar un objetivo. En el código, la clase Agent es el agente porque es responsable de decidir qué movimientos realizar en el tablero. El agente tiene una política (definida en su método move) que utiliza para seleccionar acciones, y una función de valor que utiliza para estimar qué tan buenas son las posiciones en el tablero. El agente aprende de la experiencia al actualizar su función de valor basada en las recompensas recibidas, lo que le permite mejorar sus decisiones a lo largo del tiempo.\n","\n","**Recompensa:** La recompensa es una señal que el agente recibe del entorno para evaluar la calidad de sus acciones. En el código, la recompensa se da en el método reward de la clase Game. Esta señal indica al agente si la acción que tomó condujo a un resultado positivo (ganar el juego), negativo (perder el juego) o neutral (empatar). La recompensa es fundamental en el aprendizaje por refuerzo porque guía al agente para que refuerce las acciones que conducen a resultados positivos y evite las que conducen a resultados negativos.\n","\n","**Política:** La política es la estrategia que el agente utiliza para decidir qué acción tomar en un estado dado. En el código, la política se implementa en el método move de la clase Agent. La política puede ser determinista (siempre elige la misma acción para un estado dado) o estocástica (elige acciones basadas en probabilidades). La política en este código es estocástica ya que el agente puede explorar (elegir una acción al azar) o explotar (elegir la mejor acción según su función de valor), y la probabilidad de explorar se controla con prob_exp.\n","\n","**Función de valor:** La función de valor es una estimación del agente de qué tan buena es una posición en el tablero, dadas las recompensas futuras que podría recibir. En el código, la función de valor está representada por el diccionario value_function en la clase Agent. Esta función asigna un valor numérico a cada estado posible del tablero, que se actualiza a medida que el agente recibe recompensas. La función de valor ayuda al agente a predecir qué estados son más prometedores y, por lo tanto, a elegir acciones que maximicen las recompensas futuras."],"metadata":{"id":"UMl7El40H0vH"}},{"cell_type":"code","source":["\"\"\"\n","Creación de las librerías para trabajar con este Agente para un 4 en raya\n","\"\"\"\n","\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","import pickle"],"metadata":{"id":"Ckxhz0OrtOMz","executionInfo":{"status":"ok","timestamp":1717524793963,"user_tz":240,"elapsed":821,"user":{"displayName":"Gabriel Aparicio LLanquipacha","userId":"17636720884960682861"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","execution_count":10,"metadata":{"id":"sSqli4W-Vg23","executionInfo":{"status":"ok","timestamp":1717524793964,"user_tz":240,"elapsed":9,"user":{"displayName":"Gabriel Aparicio LLanquipacha","userId":"17636720884960682861"}}},"outputs":[],"source":["class Board():\n","    def __init__(self):\n","        # El tablero debe ser de 6x7\n","        self.state = np.zeros((6, 7))\n","\n","    def valid_moves(self):\n","        return [j for j in range(7) if self.state[0, j] == 0]\n","\n","    def update(self, symbol, col):\n","        # Encuentra la fila más baja disponible en la columna seleccionada\n","        for row in range(5, -1, -1):  # Comenzar desde la parte inferior del tablero\n","            if self.state[row, col] == 0:\n","                self.state[row, col] = symbol\n","                return\n","        raise ValueError(\"Movimiento ilegal!\")\n","\n","    def is_game_over(self):\n","        # Verificar filas y columnas para encontrar cuatro en línea\n","        for c in range(7):\n","            for r in range(6):\n","                if r <= 2 and np.all(self.state[r:r+4, c] == self.state[r, c]) and self.state[r, c] != 0:\n","                    return self.state[r, c]\n","                if c <= 3 and np.all(self.state[r, c:c+4] == self.state[r, c]) and self.state[r, c] != 0:\n","                    return self.state[r, c]\n","        # Verificar diagonales\n","        for r in range(3):\n","            for c in range(4):\n","                if np.all([self.state[r+i, c+i] == self.state[r, c] for i in range(4)]) and self.state[r, c] != 0:\n","                    return self.state[r, c]\n","                if np.all([self.state[r+3-i, c+i] == self.state[r+3, c] for i in range(4)]) and self.state[r+3, c] != 0:\n","                    return self.state[r+3, c]\n","        # Si no se encuentra un ganador, retornar None\n","        return None\n","\n","    def reset(self):\n","        self.state = np.zeros((6, 7))\n","\n","class Agent:\n","    def __init__(self, alpha=0.1, prob_exp=0.3, symbol=1):\n","        self.alpha = alpha\n","        self.prob_exp = prob_exp\n","        self.value_function = {}\n","        self.positions = []\n","        self.symbol = symbol\n","\n","    def move(self, board, explore=True):\n","        valid_moves = board.valid_moves()\n","        # exploración\n","        if explore and np.random.uniform(0, 1) < self.prob_exp:\n","            # vamos a una posición aleatoria\n","            col = np.random.choice(valid_moves)\n","            return col\n","        # explotación\n","        # vamos a la posición con más valor\n","        max_value = -np.inf\n","        best_col = None\n","        for col in valid_moves:\n","            for row in range(5, -1, -1):\n","                if board.state[row, col] == 0:\n","                    next_board = board.state.copy()\n","                    next_board[row, col] = self.symbol\n","                    next_state = str(next_board.reshape(6*7))\n","                    value = self.value_function.get(next_state, 0)\n","                    if value > max_value:\n","                        max_value = value\n","                        best_col = col\n","                    break\n","        return best_col\n","\n","    def update(self, board):\n","        self.positions.append(str(board.state.reshape(6*7)))\n","\n","    def reward(self, reward):\n","        # al final de la partida (cuando recibimos la recompensa)\n","        # iteramos por todos los estados actualizando su valor en la tabla\n","        for p in reversed(self.positions):\n","            if self.value_function.get(p) is None:\n","                self.value_function[p] = 0\n","            self.value_function[p] += self.alpha * (reward - self.value_function[p])\n","            reward = self.value_function[p]\n","\n","class Game:\n","    def __init__(self, agent1, agent2):\n","        self.board = Board()\n","        self.agent1 = agent1\n","        self.agent2 = agent2\n","\n","    def selfplay(self, rounds):\n","        wins = {1: 0, 2: 0, 'draw': 0}\n","        for _ in range(rounds):\n","            self.board.reset()\n","            game_over = False\n","            turn = 0\n","            while not game_over:\n","                player = self.agent1 if turn % 2 == 0 else self.agent2\n","                col = player.move(self.board)\n","                self.board.update(player.symbol, col)\n","                player.update(self.board)\n","                winner = self.board.is_game_over()\n","                if winner is not None:\n","                    wins[winner] += 1\n","                    game_over = True\n","                elif all(self.board.state[0, :] != 0):\n","                    wins['draw'] += 1\n","                    game_over = True\n","                turn += 1\n","        return wins"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"jh9S5GThVg26","outputId":"a842618a-5d20-40cb-eed0-e966aac1466b","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1717525176147,"user_tz":240,"elapsed":382191,"user":{"displayName":"Gabriel Aparicio LLanquipacha","userId":"17636720884960682861"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Empty DataFrame\n","Columns: [estado, valor]\n","Index: []\n"]}],"source":["# Creación de agentes y juego\n","agent1 = Agent(prob_exp=0.5)\n","agent2 = Agent()\n","game = Game(agent1, agent2)\n","wins = game.selfplay(30000)\n","\n","# Guardar y mostrar la función de valor aprendida\n","funcion_de_valor = sorted(agent1.value_function.items(), key=lambda kv: kv[1], reverse=True)\n","tabla = pd.DataFrame({'estado': [x[0] for x in funcion_de_valor], 'valor': [x[1] for x in funcion_de_valor]})\n","print(tabla)\n","\n","# Guardar la función de valor en un archivo\n","with open('agente.pickle', 'wb') as handle:\n","    pickle.dump(agent1.value_function, handle, protocol=pickle.HIGHEST_PROTOCOL)"]},{"cell_type":"code","source":["tabla"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":89},"id":"mTogn9dhVPV5","executionInfo":{"status":"ok","timestamp":1717526681654,"user_tz":240,"elapsed":324,"user":{"displayName":"Gabriel Aparicio LLanquipacha","userId":"17636720884960682861"}},"outputId":"e58ce95a-b71f-44f7-fe9e-05014079c076"},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Empty DataFrame\n","Columns: [estado, valor]\n","Index: []"],"text/html":["\n","  <div id=\"df-4bea5937-0e01-434b-a27e-3fba03322be9\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>estado</th>\n","      <th>valor</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4bea5937-0e01-434b-a27e-3fba03322be9')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-4bea5937-0e01-434b-a27e-3fba03322be9 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-4bea5937-0e01-434b-a27e-3fba03322be9');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","  <div id=\"id_b3ec74bf-d64a-4805-8318-fd1917417d7c\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('tabla')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_b3ec74bf-d64a-4805-8318-fd1917417d7c button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('tabla');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"tabla","summary":"{\n  \"name\": \"tabla\",\n  \"rows\": 0,\n  \"fields\": [\n    {\n      \"column\": \"estado\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"valor\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":13}]}],"metadata":{"interpreter":{"hash":"bb9f406c0f70fca9801e60f2cbb7cd1ccff2ae2f74c58f513340bcf6cae5ecd0"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.5"},"colab":{"provenance":[{"file_id":"https://github.com/juansensio/axr/blob/master/axr/00_intro.ipynb","timestamp":1715661008364}]}},"nbformat":4,"nbformat_minor":0}